---
title: "Drugs and Jobs: The effect of unemployment on drug overdose deaths in America"
author: "Evan Arnold and Caleb Ren"
date: "12/06/2019"
bibliography: bibliography.bib
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = T, fig.height = 4, fig.align = 'center',
                      message = F, warning = F)
set.seed(139)
```

\section{Introduction and Motivation}

Overdose deaths in the US have increased dramatically since 1999. Opioid use deaths in particular have reached epidemic levels, with a 200\% increase in overdose deaths between 2000 and 2014 [@Ghertner2018]. The Centers for Disease Control and Prevention estimates that over 60,000 drug overdose deaths occurred in 2016, three times the rate of drug deaths as 1999 [@Hedegaard2018]

\section{Exploratory Data Anslysis Methods}

\subsection{Data Summary}

As a next step in our project, we collected data from the CDC in the form of the Vital Statistics Rapid Release dataset (VSRR). The VSRR data contains provisional counts of drug overdose deaths in the US as reported by agencies from all 50 states and the District of Columbia. The data is collected in on a monthly basis.

The data of import to this project is the number of deaths in each state as a result of drug overdose. Drug overdoses are counted by state agencies in acordance to World Health Organization standards, which lay out the basic guides for reporting agencies to code and classify causes of death. Drug categories that are represented in this dataset include the major drivers of the opioid epidemic like heroin (coded by T40.1), natural opioid analgesics (morphine and codeine), synthetic opioids (oxycodone, hydrocodone, oxymorphone; T40.2), methadone (T40.3), other synthetics (fentanyl, tramadol; T40.4) and other drugs like cocaine, methamphetamine, etc.

There were over 26052 data points from the VSRR dataset. Of those data points, many are individual observation of different coded deaths from different drugs; after reshaping and data cleaning, there are now 2652 individual observations. The data ranges from 2015 to 2019, with each state reporting 52 observations (once per month). Overdose deaths range from 55 deaths in the month of May 2015 in South Dakota to a high of 5697 in Pennsylvania in September of 2017.

```{r reading}
# overdose dataset 
overdose <- read.csv("data/overdose.csv")

# remove columns
bad.cols <- c("Period", "Percent.Complete", "Percent.Pending.Investigation", 
              "State.Name", "Footnote", "Footnote.Symbol", "Predicted.Value")
overdose <- overdose[,!(colnames(overdose) %in% bad.cols)]

# reshape dataframe to wide format
overdose <- reshape(overdose, idvar = c("State", "Year", "Month"), 
                    timevar = "Indicator", direction = "wide")

# proper column naames
names <- c("state", "year", "month", "overdoseDeaths", 
           "natural.semiSynthetic.synthetic.methadone", 
           "opioids", "cocaine", "stimulants", "deaths", 
           "synthetic.noMethadone", "heroin", 
           "natural.semiSynthetic.methadone", 
           "natural.semiSynthetic", "percentSpecified", 
           "methadone")
colnames(overdose) <- names

# remove aggregate statistics
overdose <- overdose[!(overdose$state %in% c("US", "YC")),]
overdose$state <- droplevels(overdose$state)

# reformat month to ordered factor
months.levels <- c("January", "February", "March", "April", "May", "June", 
                   "July", "August", "September", "October", "November", "December")
months.labels <- substr(tolower(months.levels), 1, 3)
overdose$month <- ordered(overdose$month, levels = months.levels, labels = months.labels)

# relevant columns
overdose <- overdose[,1:4]

# adding dates
overdose$dates <- as.Date(paste("01", overdose$month, overdose$year, sep =""), 
                          format = "%d%b%Y")
```

```{r censusapi}
require("censusapi")
Sys.setenv(CENSUS_KEY = '3473513fa1cde2a0539bd2449749c5a3a4fc45ce')
readRenviron("~/.Renviron")
key <- Sys.getenv("CENSUS_KEY")

# list census apis
apis <- listCensusApis()
```

Unemployment data was sourced from the Bureau of Labor Statistics. Unemployment data is published in monthly increments from the Bureau of Labor Statistics by state. Data is published beginning in 1976 and is published on the first of each month describing the previous month's unemployment rate.

There is a very specific definition of who in the labor force is considered *unemployed*. According to the BLS, those who are currently unemployed are those who are "jobless, looking for a job, and avaiable for work." People who are incarcerated, in a nursing home, or in a mental health care facility are not considered unemployed as they are not fit for work.

Using this definition, data was scraped from the BLS website and aggregated by each state and the District of Columbia. The unemployment rate in percent is given by the `unemployment` column. The lowest unemployment rate in a given state and month is Vermont in 2019 with a 2.1% unemployment rate. The highest rate is DC in 2015 with a 7.4% unemployment rate. The data itself is roughly Normally distributed with a mean of 4.2% and a median of 4.31%.

```{r overdose}
# iterate through state data files 
unemployment <- data.frame()
for (file in list.files("data/state", full.names = T)) {
  
  # state name and data
  state <- substr(basename(file), 1, 2)
  data <- read.csv(file)
  data$state <- rep(state, nrow(data))
  
  # year and month
  data$year <- sapply(data$DATE, function(x) as.numeric(substr(x, 1, 4)))
  data <- data[data$year >= 2015,]
  month <- sapply(data$DATE, function(x) as.numeric(substr(x, 6, 7)))
  month <- ordered(month, labels = months.labels)
  data$month <- month
  colnames(data)[2] <- "unemployment"
  
  # record state data
  unemployment <- rbind(unemployment, data)
}
colnames(unemployment)[2] <- "unemployment"
unemployment <- unemployment[,-1] # drop default date column

# merge datasets
overdose <- merge(overdose, unemployment, by = c("state", "year", "month"))

# order
overdose <- overdose[order(overdose$year, overdose$state, overdose$month),]
```

St. Louis Datasets
permits: housing units authorized by building permits (raw count). This is a proxy for housing development.
imports: imports in millions of dollars. This is a proxy for in-state manufactoring.
income: annual income per capita

```{r stlouis}
# convert Federal Reserve Bank of St. Louis data to long-form
stlouis <- function(wide, var.name, state_start, state_end) {
  # states to include
  states.include <- levels(overdose$state)
  
  # state names
  cols.states <- substr(colnames(wide)[-1], state_start, state_end)
  colnames(wide) <- c("date", cols.states)
  
  # filter for states
  wide <- wide[,c("date", states.include)]
  
  # expand date column
  wide$month <- ordered(months(as.POSIXct(wide$date)), 
                        levels = months.levels, 
                        labels = months.labels)
  wide$year <- as.numeric(substr(wide$date, 1, 4))
  wide <- wide[,-1] # remove original date variable
  wide <- wide[,c("year", "month", states.include)]
  
  # filter years
  wide <- wide[wide$year >= 2015 & wide$year <= 2019,]
  
  # filter months in 2019 (april is the last month in the overdose dataset in 2019)
  wide <- wide[!(wide$month > "apr" & wide$year == 2019),]
  
  # convert to longform data
  long <- reshape(wide, direction = "long", 
                  varying = states.include, 
                  v.names = var.name, 
                  idvar = c("year", "month"), 
                  times = states.include)
  
  # column name and order
  colnames(long)[3] <- "state"
  long <- long[,c("state", "year", "month", var.name)]
  
  return(long)
}

# read in data
imports <- read.table("data/imports/Imports.txt", header = T)
permits <- read.table("data/permits/Permits.txt", header = T)
income <- read.table("data/income/income.txt", header = T)

multmerge <- function(mypath) {
  filesnames <- list.files(path = "data/exports", full.names = T)
  datalist <- lapply(filesnames, function(x) {read.csv(file = x, header = T) })
  Reduce(function(d1, d2) merge(d1, d2, by = "DATE"), datalist)
}
exports <- multmerge("data/exports")
names(exports) <- c("dates", substr(names(exports)[-1], 7, 9))

# convert to long-form
permits <- stlouis(permits, "permits", 1, 2)
imports <- stlouis(imports, "imports", 7, 8)
income <- stlouis(income, "income", 1, 2)[-3]
exports <- reshape(exports, 
                   direction = "long", varying = names(exports)[-1], 
                   v.names = "exports", timevar = "state", 
                   times = names(exports)[-1])
exports$dates <- as.Date(exports$date)
exports$id <- NULL

# combine with overdose and unemployment data
overdose <- merge(overdose, permits, by = c("state", "year", "month"))
overdose <- merge(overdose, imports, by = c("state", "year", "month"))
overdose <- merge(overdose, exports, by = c("state", "dates"))
overdose <- merge(overdose, income, by = c("state", "year"))
```

Census Bureau Dataset
population: raw population

NOTE: For each state, we use the population estimate for the previous year for the entire year. This is due to a lack of available data as well as slow population growth across states. 

```{r population}
# data
population <- read.csv("data/population.csv")

# filter for 50 states and DC
population <- population[6:(nrow(population) - 1),]

# state names
abbrev <- function(state) {
  if (state == "District of Columbia") {
    return("DC")
  }
  return(state.abb[which(state.name == state)])
}
population$state <- unlist(sapply(population$NAME, abbrev))

# relevant variables
population <- population[,c("state", paste(rep("POPESTIMATE", 5), 
                                           2014:2018, sep = ""))]

# columns names
pop.cols <- as.character(2015:2019)
colnames(population) <- c("state", pop.cols)

# convert to long format
population <- reshape(population, direction = "long", 
                      varying = pop.cols, 
                      v.names = "population", 
                      times = pop.cols)
population <- population[,-4]
colnames(population)[2] <- "year"

# merge with overdose dataset
overdose <- merge(overdose, population, by = c("state", "year"))
```

R Data
region: data from R. We have to include the region for DC specifically

```{r R_data}
# region
overdose$region <- rep(NA, nrow(overdose))
for (i in 1:length(state.region)) {
  overdose$region[overdose$state == state.abb[i]] <- as.character(state.region[i])
}
overdose$region[overdose$state == "DC"] <- "South"
overdose$region <- factor(overdose$region)
```

Normalize overdose deaths, permits, and imports. These variables are now relative values per 100000 people. 

```{r normal}
# normalize raw predictors
overdose$overdoseDeaths <- (overdose$overdoseDeaths / overdose$population) * 100000
overdose$permits <- (overdose$permits / overdose$population) * 100000
overdose$imports <- (overdose$imports / overdose$population) * 100000
```

\subsection{EDA}

```{r toy}
colors_use <- sample(colors(distinct = T), size = 51, replace = F)
states <- levels(overdose$state)

plot_state <- function(v) {
  mydf <- subset(overdose, state == "AK")
  plot(mydf[,v][order(mydf$dates)] ~ mydf$dates[order(mydf$dates)],
       type = "l",
       ylim = range(overdose[,v]), 
       col = colors_use[1], xlab = "", ylab = v)
  for (i in 2:(length(states))) {
    mydf <- subset(overdose, state == states[i])
    lines(mydf[,v][order(mydf$dates)] ~ mydf$dates[order(mydf$dates)],
          col = colors_use[i])
  }
}

par(mfrow = c(2, 4), mar = c(2, 4, 1, 1))
for (v in c("overdoseDeaths", "unemployment", "permits", "imports", 
            "exports", "income", "population")) {
  plot_state(v)
}

```

```{r}
# histogram and boxplot of response
par(mfrow = c(2, 2), mar = c(2, 1, 1, 1))
hist(overdose$overdoseDeaths, main = "Histogram of Overdose Deaths", 
     xlab = "Overdose Deaths")
boxplot(overdose$overdoseDeaths, main = "Boxplot of Overdose Deaths", 
        xlab = "Overdose Deaths")

# histogram and boxplot of log-response
hist(log(overdose$overdoseDeaths), main = "Histogram of Log-Overdose Deaths", 
     xlab = "Log-Overdose Deaths")
boxplot(log(overdose$overdoseDeaths), main = "Boxplot of Log-Overdose Deaths", 
        xlab = "Log-Overdose Deaths")
```

We see that the data is much closer to a Normal distribution if we apply a log transformation.

```{r}
# response vs. unemployment
par(mfrow = c(1, 1))
plot(overdoseDeaths ~ unemployment, data = overdose,
     main = "Overdose Deaths vs. Unemployment",
     xlab = "Unemployment Percent", ylab = "Overdose Deaths",
     pch = 16, col = rgb(0, 0, 0, 0.3))
```

\subsection{Baseline Model}

```{r}
# train/test data for cross validation
samples <- sample(nrow(overdose), size = 0.8 * nrow(overdose), replace = F)
train <- overdose[samples,]
test <- overdose[-samples,]

# rmse function
rmse <- function(v1, v2) {
  return(sqrt(mean((v1 - v2)^2)))
}

# simple linear model
summary(lm1 <- lm(overdoseDeaths ~ unemployment, data = train))
lm1.train <- rmse(predict(lm1, train), train$overdoseDeaths)
lm1.test <- rmse(predict(lm1, test), test$overdoseDeaths)
```

The simple regression model has a positive coefficient for unemployment ($13.164$). With a $t$-statistic of $15.345$ ($p$-value $\approx 0$), this coefficient is very significant. The model has a positive association between unemployment and overdose deaths. 

```{r}
# observations with simple regression line
plot(overdoseDeaths ~ unemployment, data = overdose,
     main = "Overdose Deaths vs. Unemployment",
     xlab = "Unemployment Percent", ylab = "Overdose Deaths",
     col = rgb(0, 0, 0, 0.3), pch = 16)
x <- seq(min(overdose$unemployment), max(overdose$unemployment), 0.01)
y <- predict(lm1, newdata = data.frame(unemployment = x))
lines(y ~ x, col = "red", lwd = 3)
```

In order to better evaluate the importance of unemployment, we apply an ESS F-test. First, we fit two models: the first model contains all main effects and their respective quadratic versions besides unemployment. The second model contains the same predictors, except that it includes unemployment and its quadratic effect. Here we note that we are ignoring any time effect as of now. Additionally, we again verify the assumptions of a linear model via a plot of residuals vs. fitten values.

```{r polynomial}
# kitchen sink model without unemployment
polynomial1 <- lm(overdoseDeaths ~ poly(permits, 2, raw = T) + 
                    poly(imports, 2, raw = T) + poly(income, 2, raw = T) + 
                    poly(population, 2, raw = T) + poly(exports, 2, raw = T) +
                    region + state, data = train)

# kitchen sink model with unemployment
polynomial2 <- lm(overdoseDeaths ~ poly(unemployment, 2, raw = T) + 
                    poly(permits, 2, raw = T) + poly(imports, 2, raw = T) + 
                    poly(income, 2, raw = T) + poly(population, 2, raw = T) + 
                    poly(exports, 2 , raw = T) +
                    region + state, data = train)

poly1.train <- rmse(predict(polynomial1, train), train$overdoseDeaths)
poly1.test <- rmse(predict(polynomial1, test), test$overdoseDeaths)
poly2.train <- rmse(predict(polynomial2, train), train$overdoseDeaths)
poly2.test <- rmse(predict(polynomial2, test), test$overdoseDeaths)

# ESS F-test
anova(polynomial1, polynomial2)

# check model assumptions
plot(polynomial2, which = 3, pch = 19)
```

With an F-statistic of $12.208$ (with $2434$, $2$ degrees of freedom) and a corresponding p-value $<<0.001$, unemployment provides significant predictive ability. In order to be thorough, we examine the importance of unemployment in two additional ways: lasso regression, and random forrest regression. Additionally, there seems to be no underlying structure to the residuals, the residuals appear to have reasonably constant variance, and are reasonably normally distributed: the assumptions of our models (linearity, homoscedasticity of residuals, and normality) appear to be reasonably satisfied. As mentions above, we are purposefully ignoring a time effect so samples are likely correlated and thus not completely independent. We shall investigate this effect further in the analysis.

We begin with lasso regression. Due to l1 penalty of lasso regression, less important variables quickly converge to zero as the regularization parameter increases. Below, we fit a lasso model (with the same dedign matrix as that of the full quadratic kitchen sink model) with a series of possible regularization parameters. We then consider the order in which the variable coefficients shrink to zero.

```{r lasso} 
library(glmnet) # package

# design matrix and model
X <- model.matrix(polynomial2)
y <- train$overdoseDeaths
lasso.cv <- glmnet(X, y, lambda = 10^seq(-4, 4, 0.01), alpha = 1)
lasso.cv <- cv.glmnet(X, y, alpha = 1)
# feature importance plot
plot(lasso.cv$glmnet.fit, "lambda", main = "Lasso Feature Selection",
     xlab = "log(lambda)", ylab = "beta")
# matplot(log(lasso.cv$lambda, 10), t(lasso.cv$beta), type = "l",
#         main = "Lasso Feature Selection", xlab = "log(lambda)", ylab = "beta")
legend("topleft", colnames(X)[1:14], col = seq_len(ncol(X)[1:14]), fill = seq_len(14))
plot(lasso.cv$glmnet.fit, "lambda", main = "Lasso Feature Selection", 
     xlab = "log(lambda)", ylab = "Beta")

# matplot(log(lasso.cv$lambda, 10), t(lasso.cv$beta), type = "l",
        # main = "Lasso Feature Selection", xlab = "log(lambda)", ylab = "beta")
legend("topleft", colnames(X), col = seq_len(ncol(X)), fill = seq_len(ncol(X)), cex = 0.3)

# >>>>>>> 263d03a2c0a15a5037aa1601e8bb58161e4e6aa1
```

Lasso feature selection agrees with the results of the ESS F-test. Unemployment (the quadratic effect in this case) is one of the last coefficients to shrink to zero along with population. We further test this conclusion with a random forest model. Below, we fit a random forest model with all main effects and then examine it's relative feature importance.

```{r rf}
library(randomForest) # package

# random forest
rf1 <- randomForest(overdoseDeaths ~ unemployment + permits + 
                      imports + income + population + region + state, 
                    data = overdose, mtry = 3, ntree = 500)

# feature importance
varImpPlot(rf1, main = "Random Forest Feature Importance", pch = 20, 
           cex = 1.5)
```

Unemployment is relatively not important in the random forest model. 

```{r mixed}
library(lme4) # package

# mixed effects model
lmer1 <- lmer(overdoseDeaths ~ year + month + (1 | year) + (1 | month) + (1 | state), data = overdoseDeaths)
```

\section{Results}


\section{Conclusions and Decisions}

\section{References}
